{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's practise Deep Reinforcement Learning\n",
    "\n",
    "This notebook is built on the work I did at the University of Oslo. You can find the paper at [XX]\n",
    "\n",
    "You have probably been introduced to Deep Reinforcement Learning already, but if needed there are plenty of good resources on the subject that you can find online, such as [this one](https://skymind.ai/wiki/deep-reinforcement-learning).\n",
    "\n",
    "Now we can start !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some imports first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some useful libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from change_param import Param\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import param\n",
    "from gym_film.envs import make_env\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "p= Param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simulation\n",
    "\n",
    "**Let's render the simulation without any control**\n",
    "\n",
    "What we will see is what happens after waiting $\\Delta t = 200$ for waves to appear (at $t = 0$, $h = 1$ on the whole domain)\n",
    "\n",
    "- We will render from $t=0$ to $t=5$\n",
    "\n",
    "- Typically, a training episode lasts $\\Delta t=20$ in comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_first_jet = 150\n",
    "n_cpu = 1\n",
    "n_jets = 1\n",
    "p.update_dic({'n_jets': n_jets, \n",
    "              'position_first_jet': position_first_jet, \n",
    "              'n_cpu': n_cpu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=True, plot_jets=False)\n",
    "env=DummyVecEnv(envs)\n",
    "no_jet_obs = env.reset()\n",
    "\n",
    "# Duration of the rendering here\n",
    "time_simulation = 5\n",
    "\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "# Here we render\n",
    "for i in range(render_total_timesteps):\n",
    "    actions = np.array([0 for k in range(param.n_jets)]) # this means no actions\n",
    "    no_jet_obs, no_jet_rewards, no_jet_done, no_jet_info = env.step([actions]) # one environment step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, large waves are developing. We could wonder if it is possible to **control these waves** with, say, **jets** that can inject or blow mass flow into our simulation.\n",
    "\n",
    "That is actually what we will try to do, but keep in mind : the underlying equations of the simulation are **highly non-linear**, so it's not a trivial problem. \n",
    "\n",
    "Let's see a render of what a jet can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jets = 10\n",
    "JET_WIDTH_mm = 5.0\n",
    "space_between_jets = 10\n",
    "JET_MAX_POWER=5.0\n",
    "size_obs = 20\n",
    "perturbation_jets_position = [100] # let's put a perturbation jet for fun\n",
    "p.update_dic({'n_jets' : n_jets,\n",
    "              'JET_WIDTH_mm' : JET_WIDTH_mm,\n",
    "              'space_between_jets' : space_between_jets,\n",
    "              'JET_MAX_POWER':JET_MAX_POWER,\n",
    "              'size_obs':size_obs,\n",
    "             'perturbation_jets_position':perturbation_jets_position})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=True, plot_jets=True)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 10\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "for i in range(render_total_timesteps):\n",
    "    \n",
    "    # Here I put some possible behaviors for the jets\n",
    "    # Try changing that value to `checkboard` for example\n",
    "    actions_sampling = 'full_up'\n",
    "    \n",
    "    if actions_sampling == 'no_action':\n",
    "        actions = np.array([0 for k in range(param.n_jets)])\n",
    "        \n",
    "    if actions_sampling == 'full_up':\n",
    "        actions = np.array([1 for k in range(param.n_jets)])\n",
    "        \n",
    "    if actions_sampling == 'checkboard':\n",
    "        period = 6\n",
    "        actions = np.array([(-1)**((i+period*k)//period) for k in range(param.n_jets)])\n",
    "        \n",
    "    if actions_sampling == 'random':\n",
    "        actions = np.random.random(param.n_jets)*2-1\n",
    "        \n",
    "    if actions_sampling == 'pid_p':\n",
    "        alpha = 1\n",
    "        actions = np.empty(param.n_jets)\n",
    "        for k in range(param.n_jets):\n",
    "            h_at_jet = obs[0][k,-1-int(0.5*param.JET_WIDTH),0]\n",
    "            actions[k] = -np.clip(alpha*h_at_jet, -1, 1)\n",
    "        \n",
    "    obs, rewards, done, info = env.step([actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change a lot of things about the jets - size, maximum power, number and locations.\n",
    "As you can see, applying a very basic pid somehow works, but it is still vulnerable to big perturbations, and we will see that controlling this specific flow is not what's interesting about this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's introduce some Reinforcement Learning\n",
    "We will use two libraries mainly:\n",
    "- **OpenAI Gym** which will allow us to build custom **environments**. You can also play with pre-made environments, I invite you to check out their github repo, they have a [list of environments](https://github.com/openai/gym/blob/master/docs/environments.md) there with examples.\n",
    "- **Stable Baselines**, a fork of OpenAI baselines, which provides you with state-of-the-art algorithms for **agents**, such as DQN and PPO. Here is their [githug repo](https://github.com/hill-a/stable-baselines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's take a look at the environment\n",
    "I made a cleaned-up version of my custom environment so that you can see the **key attributes and methods an environment should have**. This is not only true for stable-baselines, other rl libraries work this way too (tensorforce for example).\n",
    "\n",
    "**Your environment class should have :**\n",
    "- An **`action_space`** and an **`observation_space`**, so that your agent understand what it has to deal with.\n",
    "In my case, the action space is the union of as much $[-1, 1]$ intervals as you have jets, $Action\\_space = [-1, 1]^{n_{jets}}$, and the observation space takes the values of $h$ and $q$ at the vicinity of each jet\n",
    "- A **`step`** method, which takes an action as argument, and return the next state, the reward of the step and a $Boolean$ value to indicate if the episode is finished or not\n",
    "**(all of these are in the line `return obs, reward, done, {}`)**\n",
    "\n",
    "\n",
    "- A **`reset`** method, which will, well, reset the environment and return its observation after the reset\n",
    "\n",
    "If you are really interested into how the code really works, you can find it all open source on [my github](https://github.com/vbelus/falling-liquid-film-drl), otherwise, please look for what I just described in the code below, and let's get to the learning phase !\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import spaces # you will use that to define your observation space and action space\n",
    "\n",
    "# the following imports are custom classes I made that deal with the simulation, \n",
    "# the observation and the reward\n",
    "from gym_film.envs.simulation_solver.simulation import Simulation\n",
    "from gym_film.envs.observation.observation import Observation\n",
    "from gym_film.envs.reward.reward import Reward\n",
    "\n",
    "class FilmEnv(gym.Env):\n",
    "    def __init__(self, jets_position, n_jets):\n",
    "        super(FilmEnv, self).__init__()\n",
    "        \n",
    "        self.jet_position = jets_position\n",
    "        self.n_jets = n_jets\n",
    "\n",
    "        self.simulation = Simulation()\n",
    "        self.system_state = self.simulation.get_system_state()\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # here is your action space\n",
    "        self.action_space = spaces.Box(low=-1,\n",
    "                                       high=1,\n",
    "                                       shape=(self.n_jets,),\n",
    "                                       dtype=np.float64)\n",
    "\n",
    "        # and here is your observation space\n",
    "        self.Ob = Observation(self.n_jets)\n",
    "        self.observation_space = self.Ob.observation_space\n",
    "\n",
    "        # we initialize our reward calculator\n",
    "        self.R = Reward(self.n_jets)\n",
    "\n",
    "    def reset(self):\n",
    "        # we don't hard-reset the simulation, it just keeps going\n",
    "        self.system_state = self.simulation.get_system_state()\n",
    "\n",
    "        self.current_epoch += 1\n",
    "        self.current_step = 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        # execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        # observe our environment\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        # get the reward\n",
    "        reward = self._next_reward()\n",
    "        \n",
    "        # check if we have finished simulation\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= param.nb_timestep_per_simulation\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    # Take action and update the state of the environment\n",
    "    def _take_action(self, action):\n",
    "        # get actual jet_power by multiplying by max power\n",
    "        jet_power = action * param.JET_MAX_POWER\n",
    "\n",
    "        # make simulation evolve with new jet_power\n",
    "        self.simulation.next_step(jet_power)\n",
    "\n",
    "        # get system state\n",
    "        self.system_state = self.simulation.get_system_state()\n",
    "        \n",
    "    def _next_observation(self):\n",
    "        return self.Ob.get_observation(self.system_state, self.jet_position)\n",
    "\n",
    "    def _next_reward(self):\n",
    "        return self.R.get_reward(self.system_state, self.jet_position)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try and train our first agent\n",
    "We are going to use a single jet at $x = 170$, where there are already small developed waves.\n",
    "\n",
    "Training on 10 000 environment steps, the training should last between $1$ and $2$ minutes depending on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_jets_position = [] # no more perturbation jets\n",
    "position_first_jet = 170\n",
    "# jet power\n",
    "size_obs_to_reward=40\n",
    "n_cpu = 1\n",
    "n_jets = 1\n",
    "JET_MAX_POWER=5.0\n",
    "p.update_dic({'n_jets': n_jets, \n",
    "              'position_first_jet': position_first_jet, \n",
    "              'perturbation_jets_position': perturbation_jets_position,\n",
    "              'n_cpu': n_cpu,\n",
    "              'size_obs_to_reward':size_obs_to_reward,\n",
    "              'JET_MAX_POWER': JET_MAX_POWER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "from gym_film.model.custom_mlp import CustomPolicy\n",
    "policy = CustomPolicy\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=False)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "model = PPO2(policy, env=env, n_steps=param.nb_timestep_per_simulation, verbose=1)\n",
    "\n",
    "# Let's train him for 10 000 environment steps\n",
    "n_step_training = 400*25\n",
    "model.learn(n_step_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's render it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=True, plot_jets=True)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    use_agent = True\n",
    "    if use_agent:\n",
    "        action, _states = model.predict(obs)\n",
    "    else:\n",
    "        action = [np.array([0 for k in range(param.n_jets)])]\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably see, this is not that good, a good policy would probably need more training.\n",
    "\n",
    "Bonus : try to put the jet at $x = 150$, where waves are starting developing. If you do that, maybe also reduce the maximum power of the jet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's render a pre-trained policy\n",
    "\n",
    "Here's what a policy with **60 000** steps of trainings can do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=True, plot_jets=True)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Here's a model trained on 60 000 timesteps\n",
    "model_path = '1jet3.zip'\n",
    "model = PPO2.load(model_path, env=env)\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have more jets\n",
    "\n",
    "**One jet** is probably **not enough** anyway to have a large instability zone controlled.\n",
    "\n",
    "Do you think this algorithm would still work if we wanted to put, say, 5 jets ?\n",
    "\n",
    "Let's see, by rendering a model I **trained with 5 jets**, on **60 000** timesteps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_first_jet = 170\n",
    "# jet power\n",
    "size_obs_to_reward=10\n",
    "n_jets = 5\n",
    "n_cpu = 1\n",
    "JET_MAX_POWER=5.0\n",
    "p.update_dic({'n_jets': n_jets, \n",
    "              'position_first_jet': position_first_jet,\n",
    "              'size_obs_to_reward':size_obs_to_reward,\n",
    "              'n_cpu':n_cpu,\n",
    "              'JET_MAX_POWER': JET_MAX_POWER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=True, plot_jets=True)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Here's a model trained on X timesteps\n",
    "model_path = '5jets.zip'\n",
    "model = PPO2.load(model_path, env=env)\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, **this algorithm does not scale well to an increase in the number of jets.**\n",
    "\n",
    "### Why is that ?\n",
    "\n",
    "Look at what we are doing right now when training the agent (it's a simplified vision of what's really happening)\n",
    "\n",
    "![graph](img/method1.png)\n",
    "\n",
    "Think about it, with this approach, we are implicitly telling our agent all the jets are just **one big entity**, this means two things :\n",
    "\n",
    "- Let's say you found a good policy for $10$ jets. Now, you can't even use it on an environment with $11$ jets ! You would have to **train another agent** for that because the inputs are not the same, even though you may think adding just one jet would not change the problem all that much\n",
    "\n",
    "- When training an agent on $10$ jets, the **reward** that he uses to adjust his policy is a **single value calculated on an area covering all the jets**. This means we are using **some information collected around the $1$st jet to adjust how the $10$th jet behaves** - even though this particular jet had nothing to do with what happened way behind him !\n",
    "\n",
    "So you would be right to think this method is not optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That why we are moving [**HERE**](Method M2.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
