{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some useful libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from change_param import Param\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import param\n",
    "from gym_film.envs import make_env\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "p= Param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting translational invariance with a CNN architecture\n",
    "\n",
    "We just said earlier that our policy should not be that different with either $10$ or $11$ jets.\n",
    "\n",
    "That's because of the **translational invariance** of the simulation. \n",
    "\n",
    "Indeed, the physics of our simulation are the same at $x = 50$ and at $x = 200$, which means that **our jets should have more or less the same behavior wherever they are.**\n",
    "\n",
    "Remember how our agent uses a neural network to transform an **observation** into an **action** ?\n",
    "\n",
    "We used to concatenate the observation of each jet into one single observation, but what we could do is apply the same transformation on each observations so that our policy is the same for each jet.\n",
    "\n",
    "Such a model is simplified on this figure right here:\n",
    "\n",
    "![graph2](img/method2.png)\n",
    "\n",
    "### A convolutional trick\n",
    "\n",
    "Using **Convolution Neural Networks** allow us to have the exact same transformations on each row of our input. This means that if **each** of our jets had the **same observation**, they would be given the **same action** on the next step (not exactly the same actually because we use a stochastic policy, but the output of our neural network would be the same for each jet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train an agent with $5$ jets with this new trick\n",
    "We will use the same parameters as in the previous failed training\n",
    "\n",
    "Training on $40 000$ environment steps, the training should last around $10$ minutes. Go take a coffee or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_first_jet = 170\n",
    "# jet power\n",
    "size_obs_to_reward=10\n",
    "n_jets = 5\n",
    "n_cpu = 1\n",
    "JET_MAX_POWER=5.0\n",
    "p.update_dic({'n_jets': n_jets, \n",
    "              'position_first_jet': position_first_jet,\n",
    "              'size_obs_to_reward':size_obs_to_reward,\n",
    "              'n_cpu':n_cpu,\n",
    "              'JET_MAX_POWER': JET_MAX_POWER})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "from stable_baselines import PPO2\n",
    "from gym_film.model.custom_shared_mlp import CustomPolicy\n",
    "policy = CustomPolicy\n",
    "\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=False)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "model = PPO2(policy, env=env, n_steps=param.nb_timestep_per_simulation, verbose=1)\n",
    "\n",
    "# Let's train him for 40 000 environment steps\n",
    "n_step_training = 400*100 # 1 episode is 400 steps\n",
    "model.learn(n_step_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's render it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "envs = make_env.make_env('1env_njet', param.n_jets, param.jets_position, render=True, plot_jets=True)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    use_agent = True\n",
    "    if use_agent:\n",
    "        action, _states = model.predict(obs)\n",
    "    else:\n",
    "        action = [np.array([0 for k in range(param.n_jets)])]\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than before, right ? (or not ? The learning process is not deterministic, so it can be pretty bad still)\n",
    "\n",
    "But remember what we said earlier about **using a single reward for all the jets** - we are still doing that here.\n",
    "\n",
    "But we can fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets go to the [next and final notebook](Method M3.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
