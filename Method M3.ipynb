{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some useful libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from change_param import Param\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import param\n",
    "from gym_film.envs import make_env\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "p= Param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting locality by making each jet a separate environment\n",
    "\n",
    "### Parallelism in DRL\n",
    "\n",
    "One of the nice things you can do in DRL to accelerate learning is **using several cloned environments** executing in parallel to train the **same agent**.\n",
    "\n",
    "**These environments are expected to be the same**, except they will go in different directions because of the policy being stochastic, allowing the agent to explore more trajectories in a quicker way than with a single environment.\n",
    "\n",
    "### In our case\n",
    "\n",
    "Now we will use that. But **we will not actually use cloned environments** of the environment we used before. That would mean duplicating the simulation itself, which would be very compute-intensive, the simulation being the bottleneck of our computation.\n",
    "\n",
    "Instead, we will make an environment for each jet, **\"tricking\" the agent into thinking that every one of our jets is the same entity !** The agent doesn't need to understand the actual reality of the simulation.\n",
    "\n",
    "I will spare you the details of the implementation, but let's see a figure that simplifies what's going on here:\n",
    "\n",
    "![m3](img/method3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have some training going\n",
    "\n",
    "Let's try something - we want **a policy that can work when we use a large number of jets.**\n",
    "\n",
    "Then, let's have two jets:\n",
    "- one at $x=150$, where there are very small waves starting developping\n",
    "- one at $x=200$, where there are already large waves \n",
    "\n",
    "This way, our agent will see quite **different trajectories** from these two jets, and will learn a policy that can adapt to both situations.\n",
    "\n",
    "We can then see how good it is on more jets ($10+$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jets = 2\n",
    "position_first_jet = 150\n",
    "space_between_jets = 50\n",
    "# jet power\n",
    "size_obs_to_reward=20\n",
    "JET_MAX_POWER=5.0\n",
    "p.update_dic({'n_jets': n_jets, \n",
    "              'position_first_jet': position_first_jet,\n",
    "              'space_between_jets': space_between_jets,\n",
    "              'size_obs_to_reward': size_obs_to_reward,\n",
    "              'JET_MAX_POWER': JET_MAX_POWER})\n",
    "\n",
    "port = 12000 # we are using sockets in the implementation of this method to communicate between\n",
    "             # the simulation and the environments, and they need a communication port, but don't mind that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port+=1\n",
    "\n",
    "from gym_film.envs import make_env\n",
    "from stable_baselines import PPO2\n",
    "from gym_film.model.custom_mlp import CustomPolicy\n",
    "policy = CustomPolicy\n",
    "\n",
    "envs = make_env.make_env('1env_1jet', param.n_jets, param.jets_position, render=False, port=port)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "model = PPO2(policy, env=env, n_steps=param.nb_timestep_per_simulation, verbose=1)\n",
    "\n",
    "# Let's train him for 40000 environment steps\n",
    "n_step_training = 800*50 # 1 episode is 800 steps now, because we now have two environments\n",
    "                         #  that both need 400 steps to complete an episode\n",
    "model.learn(n_step_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_film.envs import make_env\n",
    "envs = make_env.make_env('1env_1jet', param.n_jets, param.jets_position, render=True, plot_jets=True)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    use_agent = True\n",
    "    if use_agent:\n",
    "        action, _states = model.predict(obs)\n",
    "    else:\n",
    "        action = [np.array([0 for k in range(param.n_jets)])]\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transfer that policy to a different case\n",
    "\n",
    "As I said before, with this method **the number of jets doesn't matter**, so let's put more and see how our model does !\n",
    "\n",
    "I tried with 20 jets, with no space between them, let's see the render :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jets = 20\n",
    "space_between_jets = 5\n",
    "p.update_dic({'n_jets': n_jets,\n",
    "              'space_between_jets': space_between_jets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port+=1\n",
    "from gym_film.envs import make_env\n",
    "envs = make_env.make_env('1env_1jet', param.n_jets, param.jets_position, render=True, plot_jets=True, port=port)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    use_agent = True\n",
    "    if use_agent:\n",
    "        action, _states = model.predict(obs)\n",
    "    else:\n",
    "        action = [np.array([0 for k in range(param.n_jets)])]\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good policy\n",
    "\n",
    "Let's see what a good policy can do !\n",
    "\n",
    "This one was trained with **20 jets**, without any space between them.\n",
    "\n",
    "The jets' maximum power was $5.0$, and the agent was trained during $1.2$M steps, which means $150$ episodes of the simulation. However, the training reached a reward plateau after the $30$th episode, showing no improvement after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jets = 15\n",
    "space_between_jets = 10\n",
    "JET_MAX_POWER=1\n",
    "p.update_dic({'n_jets': n_jets,\n",
    "              'space_between_jets': space_between_jets,\n",
    "              'JET_MAX_POWER': JET_MAX_POWER})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I don't even use the same parameters as during the training, but **the results are even better** this way ! You can try using the training's parameters in comparison.\n",
    "\n",
    "This lets me think we could change dynamically the maximum power of the jets during training or validation phase to have better results. \n",
    "\n",
    "Anyway, let's see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port+=1\n",
    "from gym_film.envs import make_env\n",
    "envs = make_env.make_env('1env_1jet', param.n_jets, param.jets_position, render=True, plot_jets=True, port=port)\n",
    "env=DummyVecEnv(envs)\n",
    "obs = env.reset()\n",
    "\n",
    "# Here's a model trained on X timesteps\n",
    "model_path = 'm3.zip'\n",
    "model = PPO2.load(model_path, env=env)\n",
    "\n",
    "# Duration of the rendering here - \n",
    "# you can increase it to see how the control adapt to big waves created by a perturbation jet\n",
    "time_simulation = 20\n",
    "render_total_timesteps = int(time_simulation/param.simulation_step_time)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(render_total_timesteps):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is it for me\n",
    "\n",
    "I hope you liked this little practical exercice, and that you now want to know more about DRL. It's a very quickly growing field, and one that shows very promising results already."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
